1. 서버 확인 
10.178.178.69     SKI-ODS.ipf.net         SKI-ODS10.178.178.73     SKYTALEMETADB.ipf.net   SKYTALEMETADB10.178.178.98     ski-DN1.ipf.net         ski-DN110.178.178.111    ski-DN2.ipf.net         ski-DN210.178.178.115    ski-DN3.ipf.net         ski-DN310.178.178.94     ski-NN1.ipf.net         ski-NN110.178.178.68     ski-analytic1.ipf.net   ski-analytic1
2. 계정 설정 
### app server ( API-ANALYTIC1 ) ####ip   : 10.178.178.68계정 : app_dev / skytale123!
### hadoop server #### hdfs / chrlPW4321Ambari 로그인 계정 : admin / admin 3. oozie DB 셋팅CREATE USER 'oozie'; CREATE DATABASE 'oozie';
GRANT ALL ON oozie.* TO 'oozie'@'localhost'  IDENTIFIED BY 'oozie';GRANT ALL ON oozie.* TO 'oozie'@'%'  IDENTIFIED BY 'oozie'; FLUSH PRIVILEGES; 
4. oozie proxy user 추가
ambari를 통한 oozie-site 추가 oozie.service.ProxyUserService.proxyuser.app_dev.hosts=*oozie.service.ProxyUserService.proxyuser.app_dev.groups=*oozie.service.ProxyUserService.proxyuser.dpcore.hosts=*oozie.service.ProxyUserService.proxyuser.dpcore.groups=*
5. 실행 테스트 (1) mapreduce  hadoop jar /usr/hdp/2.6.1.0-129/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen  1000000 /teraInputhadoop jar /usr/hdp/2.6.1.0-129/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort /teraInput /teraOutput
(2) spark spark-submit --class org.apache.spark.examples.SparkPi --master local[8] /usr/hdp/2.6.1.0-129/spark/lib/spark-examples-1.6.3.2.6.1.0-129-hadoop2.7.3.2.6.1.0-129.jar  100
6. workflow test 환경 구축 (1) sqoop용 디렉토리 생성 target DB : SKI-NN1 
mysql> use test;  mysql> CREATE TABLE `STATISTICS_BY_AGE` (`AGE` int(11) DEFAULT NULL, `TOTAL_AMOUNT` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=latin1 COMMENT='Statistics per Age'; mysql> CREATE TABLE `STATISTICS_BY_BUSINESS_TYPE` (`BUSINESS_TYPE` varchar(255) DEFAULT NULL, `TOTAL_AMOUNT` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=latin1 COMMENT='Statistics per Business Type'; mysql> CREATE TABLE `STATISTICS_BY_SEX` (`SEX` varchar(255) DEFAULT NULL, `TOTAL_AMOUNT` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=latin1 COMMENT='Statistics per User Sex';
(2) hive data loading 
sample data : example.tar.gz
hadoop fs -mkdir -p /user/coreuser hadoop fs -put example ./data/card_history 
### sample hive data ###cardHistory : /user/corepuser/examples/data/card_historyuser        : /user/corepuser/examples/data/usermerchant    : /user/corepuser/examples/data/merchant
7. install config 
app path : /usr/app/dataapilog path : /var/log/dataapi 
#### Log 경로 수정 #####log4j.properties  core-start.sh  
### hdfs test user dir 추가 hadoop fs -chown -R  corepuser:corepuser /user/corepuserhadoop fs -mkdir -p  /user/app_devhadoop fs -chown -R  app_dev:app_dev /user/app_dev
### share lib 설정 ###/usr/hdp/current/oozie/bin/oozie-setup.sh sharelib create -fs hdfs://<namenode>:8020oozie admin –oozie http://oozie_server:11000/oozie -sharelibupdate
### SQOOP HOME 설정 ###JRE가 아닌 JDK가 설치되어 있어야 함 export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
### config 수정 ###
## config-api-server.propertieshttp.port=7070
### config-workflow.propertieswfs.jdbc.url=jdbc:mysql://ski-analytic1:3306/
oozie.jdbc.driverClassName=com.mysql.jdbc.Driveroozie.jdbc.database=oozieoozie.jdbc.url=jdbc:mysql://ski-NN1:3306/oozie?useUnicode=true&characterSetResults=utf8&useOldAliasMetadataBehavior=trueoozie.jdbc.username=oozieoozie.jdbc.password=oozieoozie.jdbc.maxPoolSize=50oozie.jdbc.minPoolSize=1oozie.jdbc.maxStatements=100oozie.jdbc.testConnection=true
#hibernate.hbm2ddl.auto=create   // api 서버를 최초실행할 경우hibernate.hbm2ddl.auto=validate  // db schema가 배포된 이후 , api 서버를 운영시할 시의 설정 
yarn.timeline.address=http://ski-NN1:8188yarn.rm.addr=http://ski-NN1:8088mr.jobhistory.address=http://ski-NN1:19888

### INIT-DATA-DB ###CREATE DATABASE `dpcore_workflow`;CREATE USER `wfs`;GRANT ALL PRIVILEGES ON `dpcore_workflow`.* to `wfs`@`localhost` IDENTIFIED BY 'wfs' WITH GRANT OPTION;GRANT ALL PRIVILEGES ON `dpcore_workflow`.* to `wfs`@`%` IDENTIFIED BY 'wfs' WITH GRANT OPTION;
FLUSH PRIVILEGES;
### WFS_DB-INIT.sql 
# 접속 정보 확인 및 수정 필요 #INSERT INTO WFS_CONFIG (type, fixed, sort, created_time, name, value) VALUES ('hdfs', true,  1, now(), 'fs.defaultFS', 'hdfs://SKI-NN1:8020'); INSERT INTO WFS_CONFIG (type, fixed, sort, created_time, name, value) VALUES ('yarn', true,  1, now(), 'yarn.resourcemanager.address', 'SKI-NN1:8050'); INSERT INTO WFS_CONFIG (type, fixed, sort, created_time, name, value) VALUES ('yarn', true,  2, now(), 'mapreduce.jobhistory.webapp.address', 'SKI-NN1:19888'); INSERT INTO WFS_CONFIG (type, fixed, sort, created_time, name, value) VALUES ('oozie', true, 1, now(), 'oozie.url',              'http://SKI-NN1:11000/oozie'); INSERT INTO WFS_CONFIG (type, fixed, sort, created_time, name, value) VALUES ('oozie', true, 2, now(), 'oozie.libpath',          '/user/oozie/share/lib/lib_20170727185035'); INSERT INTO WFS_CONFIG (type, fixed, sort, created_time, name, value) VALUES ('oozie', true, 3, now(), 'oozie.notification.url', 'http://ski-analytic1:7070');
8. API 실행 및 Log 확인cd ${BASE_DIR}/skcore-server.log
### 실행 ###./bin/core-start.sh 
### log 확인 ### ${BASE_LOG_DIR}/skcore-server.log
9. 실행 확인
curl -T  sample1.txt  -X POST http://ski-analytic1:7070/api/v1/workflow?requestUser=corepusercurl -T  sample2.txt  -X PUT  http://ski-analytic1:7070/api/v1/workflow/run/1?requestUser=corepuser
## sample1 ##{"name":"card_statistics_workflow","category":"statistics","alias":"sqoop-hive-workflow","description":"Get card stastics","appName":"hive-sqoop-app","xml":"<workflow-app xmlns='uri:oozie:workflow:0.5' name='hive-sqoop'> <start to='hive-node' /> <action name='hive-node'> <hive xmlns='uri:oozie:hive-action:0.6'> <job-tracker>${jobTracker}</job-tracker> <name-node>${nameNode}</name-node> <configuration> <property> <name>mapred.job.queue.name</name> <value>${queueName}</value> </property> </configuration> <query> CREATE EXTERNAL TABLE IF NOT EXISTS CARD_HISTORY ( TRANSACTION_DATE STRING, TRANSACTION_ID STRING, TRANSACTION_TYPE STRING, CARD_NUMBER STRING, CARD_OWNER STRING, EXPIRE_DATE STRING, AMOUNT INT, MERCHANT_ID STRING, LOCATION_X INT, LOCATION_Y INT ) COMMENT 'Card History Table' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '${cardHistory}'; CREATE EXTERNAL TABLE IF NOT EXISTS USERS ( USER_ID STRING, USER_NAME STRING, AGE INT, SEX STRING, ADDRESS STRING ) COMMENT 'User Table' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '${user}'; CREATE EXTERNAL TABLE IF NOT EXISTS MERCHANT ( MERCHANT_ID STRING, MERCHANT_NAME STRING, BUSINESS_TYPE STRING ) COMMENT 'Merchant Table' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '${merchant}'; DROP TABLE STATISTICS_BY_BUSINESS_TYPE; DROP TABLE STATISTICS_BY_SEX; DROP TABLE STATISTICS_BY_AGE; CREATE TABLE IF NOT EXISTS STATISTICS_BY_BUSINESS_TYPE( BUSINESS_TYPE STRING, TOTAL_AMOUNT INT ) COMMENT 'Statistics per Business Type' ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; CREATE TABLE IF NOT EXISTS STATISTICS_BY_SEX( SEX STRING, TOTAL_AMOUNT INT ) COMMENT 'Statistics per User Sex' ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; CREATE TABLE IF NOT EXISTS STATISTICS_BY_AGE( AGE INT, TOTAL_AMOUNT INT ) COMMENT 'Statistics per Age' ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; INSERT INTO TABLE STATISTICS_BY_BUSINESS_TYPE SELECT M.BUSINESS_TYPE, SUM(C.AMOUNT) AS TOTAL_AMOUNT FROM CARD_HISTORY C JOIN MERCHANT M ON (C.MERCHANT_ID = M.MERCHANT_ID) GROUP BY M.BUSINESS_TYPE; INSERT INTO TABLE STATISTICS_BY_SEX SELECT U.SEX, SUM(C.AMOUNT) AS TOTAL_AMOUNT FROM CARD_HISTORY C JOIN USERS U ON (C.CARD_OWNER = U.USER_ID) GROUP BY U.SEX; INSERT INTO TABLE STATISTICS_BY_AGE SELECT U.AGE, SUM(C.AMOUNT) AS TOTAL_AMOUNT FROM CARD_HISTORY C JOIN USERS U ON (C.CARD_OWNER = U.USER_ID) GROUP BY U.AGE; </query> </hive> <ok to='fork' /> <error to='fail' /> </action> <fork name='fork'> <path start='sqoop-node-1' /> <path start='sqoop-node-2' /> <path start='sqoop-node-3' /> </fork> <action name='sqoop-node-1'> <sqoop xmlns='uri:oozie:sqoop-action:0.4'> <job-tracker>${jobTracker}</job-tracker> <name-node>${nameNode}</name-node> <configuration> <property> <name>mapred.job.queue.name</name> <value>${queueName}</value> </property> </configuration> <command>export --connect jdbc:mysql://${db}/${table} --driver com.mysql.jdbc.Driver --username ${dbuser} --password ${dbpassword} --table STATISTICS_BY_BUSINESS_TYPE --hcatalog-table STATISTICS_BY_BUSINESS_TYPE --skip-dist-cache</command> </sqoop> <ok to='join' /> <error to='fail' /> </action> <action name='sqoop-node-2'> <sqoop xmlns='uri:oozie:sqoop-action:0.4'> <job-tracker>${jobTracker}</job-tracker> <name-node>${nameNode}</name-node> <configuration> <property> <name>mapred.job.queue.name</name> <value>${queueName}</value> </property> </configuration> <command>export --connect jdbc:mysql://${db}/${table} --driver com.mysql.jdbc.Driver --username ${dbuser} --password ${dbpassword} --table STATISTICS_BY_SEX --hcatalog-table STATISTICS_BY_SEX --skip-dist-cache</command> </sqoop> <ok to='join' /> <error to='fail' /> </action> <action name='sqoop-node-3'> <sqoop xmlns='uri:oozie:sqoop-action:0.4'> <job-tracker>${jobTracker}</job-tracker> <name-node>${nameNode}</name-node> <configuration> <property> <name>mapred.job.queue.name</name> <value>${queueName}</value> </property> </configuration> <command>export --connect jdbc:mysql://${db}/${table} --driver com.mysql.jdbc.Driver --username ${dbuser} --password ${dbpassword} --table STATISTICS_BY_AGE --hcatalog-table STATISTICS_BY_AGE --skip-dist-cache</command> </sqoop> <ok to='join' /> <error to='fail' /> </action> <join name='join' to='end' /> <kill name='fail'> <message>Hive failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message> </kill> <end name='end' /> </workflow-app>","libPath":null,"useSystemLibPath":true}
## sample2 ##{"db":"ski-nn1:3306","table":"test","dbuser":"oozie","dbpassword":"oozie","queueName":"default","cardHistory":"/user/corepuser/examples/data/card_history","user":"/user/corepuser/examples/data/user","merchant":"/user/corepuser/examples/data/merchant","oozie.action.sharelib.for.sqoop":"sqoop,hive,hcatalog"}




